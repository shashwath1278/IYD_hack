#!/usr/bin/env python3
"""
ENHANCED HYBRID RAG + SEMANTIC PATTERNS + ADVANCED CHARACTER VALIDATION
Complete implementation with character confusion detection, negation processing, and bias-neutral prompting
"""

import pandas as pd
import numpy as np
import re
import time
import json
import logging
import os
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings("ignore")

# Import for dynamic regex system
try:
    import inflect
    import nltk
    from nltk.stem import WordNetLemmatizer
    DYNAMIC_REGEX_AVAILABLE = True
except ImportError:
    DYNAMIC_REGEX_AVAILABLE = False
    warnings.warn("Dynamic regex system requires inflect and nltk packages. Install with: pip install inflect nltk")

# Enhanced imports for character validation
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    warnings.warn("Advanced character validation requires transformers and torch. Install with: pip install torch transformers")

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    logger.error("Groq not available. Install with: pip install groq")

class SanskritCharacterValidator:
    """Advanced character validation system to prevent character confusion errors"""
    
    def __init__(self):
        # Character hierarchy and relationships
        self.character_hierarchy = {
            "protagonists": ["Rama", "Lakshmana", "Sita"],
            "supporting": ["Bharata", "Shatrughna", "Hanuman", "Sugriva"],
            "antagonists": ["Ravana", "Kumbhakarna", "Indrajit"],
            "mentors": ["Vasishtha", "Vishwamitra"],
            "royalty": ["Dasaratha", "Janaka", "Kaikeyi", "Kausalya"]
        }
        
        # Critical: Handle Sanskrit name variations
        self.character_aliases = {
            "rama": ["raghava", "raghunatha", "dasharathi", "ramachandra", "prince rama", "lord rama"],
            "bharata": ["bharatha", "bharat", "prince bharata"],
            "ravana": ["dashanan", "lankesh", "ravanaasura", "demon king", "demon king ravana"],
            "hanuman": ["anjaneya", "maruti", "pawanputra", "bajrangbali", "monkey warrior"],
            "sita": ["seetha", "janaki", "vaidehi", "maithili", "princess sita"],
            "lakshmana": ["lakshman", "saumitri", "prince lakshmana"],
            "sugriva": ["monkey king", "vanara king"],
            "dasaratha": ["dasharath", "king dasaratha", "raja dasaratha"],
            "kumbhakarna": ["kumbhakaran", "giant demon"]
        }
        
        # Define valid character-action relationships (CRITICAL FIX)
        self.valid_relationships = {
            "rama": {
                "can_meet": ["lakshmana", "sita", "hanuman", "sugriva", "ravana", "bharata", "vasishtha"],
                "primary_actions": ["rescue", "defeat", "rule", "exile", "return", "marry", "mourn"],
                "cannot_actions": ["betray", "refuse_duty", "ignore_dharma"],
                "relationships": {"sita": "husband", "lakshmana": "elder_brother", "bharata": "elder_brother"}
            },
            "bharata": {
                "can_meet": ["rama", "lakshmana", "shatrughna", "kaikeyi", "dasaratha"],
                "cannot_meet": ["sugriva", "hanuman", "ravana"],  # Key validation rule
                "primary_actions": ["rule", "request", "wait", "serve", "protect_kingdom"],
                "cannot_actions": ["meet_sugriva", "fight_demons", "cross_ocean"],
                "relationships": {"rama": "younger_brother", "kaikeyi": "mother"}
            },
            "hanuman": {
                "can_meet": ["rama", "lakshmana", "sita", "sugriva", "ravana", "bharata"],
                "primary_actions": ["fly", "leap", "search", "fight", "serve", "burn_lanka"],
                "cannot_actions": ["betray_rama", "refuse_help"],
                "relationships": {"rama": "devotee", "sugriva": "ally", "vayu": "father"}
            },
            "lakshmana": {
                "can_meet": ["rama", "sita", "hanuman", "sugriva", "bharata", "ravana"],
                "primary_actions": ["accompany", "protect", "fight", "serve"],
                "relationships": {"rama": "younger_brother", "sita": "sister_in_law"}
            },
            "sita": {
                "can_meet": ["rama", "lakshmana", "ravana", "hanuman", "janaka"],
                "primary_actions": ["marry", "accompany", "test_purity", "enter_earth"],
                "cannot_actions": ["betray_rama", "fight_battles"],
                "relationships": {"rama": "wife", "janaka": "father", "ravana": "abductor"}
            }
        }
        
        # Common character confusion patterns (RESEARCH-BASED)
        self.confusion_patterns = {
            "bharata_rama_substitution": {
                "pattern": r"bharata.*(?:meets?|encounter|fight).*(?:sugriva|hanuman|ravana)",
                "error_type": "character_substitution",
                "correction": "Bharata never meets Sugriva, Hanuman, or fights demons - these are Rama's actions"
            },
            "supporting_protagonist_confusion": {
                "pattern": r"(?:bharata|shatrughna).*(?:rescue|defeat|main).*(sita|ravana)",
                "error_type": "role_elevation", 
                "correction": "Only Rama rescues Sita and defeats Ravana - supporting characters have different roles"
            },
            "timeline_confusion": {
                "pattern": r"bharata.*(?:exile|forest).*(?:with|during).*rama",
                "error_type": "timeline_error",
                "correction": "Bharata rules Ayodhya during Rama's exile - he doesn't accompany Rama to forest"
            }
        }
    
    def normalize_character_name(self, text: str) -> str:
        """Normalize character names to canonical form"""
        text_lower = text.lower()
        
        for canonical, aliases in self.character_aliases.items():
            for alias in aliases:
                if alias in text_lower:
                    return canonical
        
        return text_lower
    
    def validate_character_action(self, character: str, action: str, target: str = None, context: str = "") -> Dict[str, Any]:
        """Validate if character-action combination is valid in Ramayana"""
        if not character:
            return {"valid": True, "confidence": 0.5, "reason": "No character specified"}
            
        character = self.normalize_character_name(character)
        
        if character not in self.valid_relationships:
            return {"valid": True, "confidence": 0.6, "reason": f"Unknown character: {character}"}
        
        char_info = self.valid_relationships[character]
        
        # Critical validation: Check impossible relationships
        if target:
            target = self.normalize_character_name(target)
            if target in char_info.get("cannot_meet", []):
                return {
                    "valid": False,
                    "confidence": 0.95,
                    "reason": f"{character.title()} never meets {target.title()} in Ramayana",
                    "error_type": "character_substitution",
                    "correction": f"This appears to be character confusion - check if Rama was meant instead of {character.title()}"
                }
        
        # Check impossible actions
        action_lower = action.lower()
        for impossible_action in char_info.get("cannot_actions", []):
            if impossible_action.replace("_", " ") in action_lower:
                return {
                    "valid": False,
                    "confidence": 0.90,
                    "reason": f"{character.title()} cannot perform action: {action}",
                    "error_type": "invalid_action",
                    "correction": f"This action is not consistent with {character.title()}'s role in Ramayana"
                }
        
        # Check for confusion patterns in full context
        context_lower = context.lower()
        for pattern_name, pattern_info in self.confusion_patterns.items():
            if re.search(pattern_info["pattern"], context_lower):
                return {
                    "valid": False,
                    "confidence": 0.88,
                    "reason": pattern_info["correction"],
                    "error_type": pattern_info["error_type"],
                    "pattern_matched": pattern_name
                }
        
        return {"valid": True, "confidence": 0.85, "reason": "Character-action combination appears valid"}
    
    def extract_character_info(self, text: str) -> Dict[str, Any]:
        """Extract characters and their actions from text"""
        text_lower = text.lower()
        
        # Find characters
        characters_found = []
        for canonical, aliases in self.character_aliases.items():
            for alias in aliases:
                if alias in text_lower:
                    characters_found.append(canonical)
                    break
        
        # Extract actions (verbs)
        action_words = re.findall(r'\b(?:meets?|encounter|fight|rescue|defeat|rule|marry|serve|protect|accompany)\w*\b', text_lower)
        
        return {
            "characters": list(set(characters_found)),
            "actions": action_words,
            "character_count": len(set(characters_found))
        }

class ReligiousNegationProcessor:
    """Enhanced negation processing for religious texts"""
    
    def __init__(self):
        # Sanskrit/Hindi negation cues
        self.negation_cues = [
            "not", "never", "no", "none", "neither", "nor", "wasn't", "weren't", "doesn't", "didn't",
            "cannot", "could not", "would not", "should not",
            "à¤¨à¤¹à¥€à¤‚", "à¤¨", "à¤®à¤¤", "à¤¨à¤¾", "à¤•à¤­à¥€ à¤¨à¤¹à¥€à¤‚"
        ]
        
        # Scope terminators
        self.scope_terminators = [
            "but", "however", "yet", "though", "except", "although",
            "à¤²à¥‡à¤•à¤¿à¤¨", "à¤ªà¤°à¤‚à¤¤à¥", "à¤•à¤¿à¤‚à¤¤à¥"
        ]
        
        # Known Ramayana facts for validation
        self.known_facts = {
            "rama was exiled for fourteen years": True,
            "ravana had ten heads and twenty arms": True,
            "hanuman was son of wind god vayu": True,
            "bharata ruled ayodhya during rama's exile": True,
            "sita was daughter of king janaka": True,
            "lakshmana was rama's younger brother": True,
            "sugriva became king after vali's death": True,
            "hanuman could fly and leap great distances": True,
            "rama used brahmastra in battle": True,
            "kumbhakarna slept for six months": True,
            
            # Common false statements
            "rama was born in lanka": False,
            "ravana had ten arms": False,
            "bharata fought against rama": False,
            "hanuman betrayed rama": False,
            "sita was ravana's daughter": False,
            "rama refused to go into exile": False,
            "lakshmana was not related to rama": False
        }
    
    def process_negated_statement(self, statement: str) -> Dict[str, Any]:
        """Process statements with negation, especially 'Not:' prefix"""
        statement = statement.strip()
        
        # Handle "Not:" prefix specifically (CRITICAL FIX)
        if statement.lower().startswith("not:"):
            core_statement = statement[4:].strip()
            
            # Extract the factual claim
            fact_validity = self.verify_core_fact(core_statement)
            
            # Reverse the validity for negated statement
            if fact_validity["is_true"]:
                return {
                    "statement_validity": False,  # Negation of true fact = false
                    "reasoning": f"The core statement '{core_statement}' is TRUE in Ramayana, so 'Not: {core_statement}' is FALSE",
                    "confidence": fact_validity["confidence"],
                    "negation_type": "prefix_negation",
                    "core_fact": core_statement,
                    "core_fact_validity": True
                }
            else:
                return {
                    "statement_validity": True,   # Negation of false fact = true
                    "reasoning": f"The core statement '{core_statement}' is FALSE in Ramayana, so 'Not: {core_statement}' is TRUE",
                    "confidence": fact_validity["confidence"],
                    "negation_type": "prefix_negation", 
                    "core_fact": core_statement,
                    "core_fact_validity": False
                }
        
        # Handle embedded negation
        return self.process_embedded_negation(statement)
    
    def verify_core_fact(self, fact: str) -> Dict[str, Any]:
        """Verify factual claim against knowledge base"""
        fact_lower = fact.lower().strip()
        
        # Direct lookup in known facts
        if fact_lower in self.known_facts:
            return {
                "is_true": self.known_facts[fact_lower],
                "confidence": 0.95,
                "source": "direct_lookup"
            }
        
        # Fuzzy matching for similar facts
        best_match = None
        best_score = 0
        
        for known_fact, validity in self.known_facts.items():
            # Simple word overlap scoring
            fact_words = set(fact_lower.split())
            known_words = set(known_fact.split())
            overlap = len(fact_words.intersection(known_words))
            score = overlap / max(len(fact_words), len(known_words))
            
            if score > best_score and score > 0.6:
                best_score = score
                best_match = (known_fact, validity)
        
        if best_match:
            return {
                "is_true": best_match[1],
                "confidence": 0.8 * best_score,
                "source": "fuzzy_match",
                "matched_fact": best_match[0]
            }
        
        # Default for unknown facts
        return {
            "is_true": False,
            "confidence": 0.5,
            "source": "unknown_fact"
        }
    
    def process_embedded_negation(self, statement: str) -> Dict[str, Any]:
        """Process negation embedded within statements"""
        statement_lower = statement.lower()
        
        # Check for negation cues
        negation_present = any(cue in statement_lower for cue in self.negation_cues)
        
        if not negation_present:
            return {
                "statement_validity": None,  # No negation detected
                "reasoning": "No negation detected in statement",
                "negation_type": "none"
            }
        
        # Extract the scope of negation
        negation_scope = self.extract_negation_scope(statement)
        
        return {
            "statement_validity": None,  # Requires further analysis
            "reasoning": f"Embedded negation detected: {negation_scope}",
            "negation_type": "embedded",
            "negation_scope": negation_scope,
            "requires_context_analysis": True
        }
    
    def extract_negation_scope(self, statement: str) -> str:
        """Extract the scope of negation in a statement"""
        # Find negation cue position
        statement_lower = statement.lower()
        
        for cue in self.negation_cues:
            if cue in statement_lower:
                cue_pos = statement_lower.find(cue)
                
                # Extract text after negation cue until scope terminator
                after_negation = statement[cue_pos + len(cue):].strip()
                
                # Check for scope terminators
                for terminator in self.scope_terminators:
                    if terminator in after_negation.lower():
                        term_pos = after_negation.lower().find(terminator)
                        scope = after_negation[:term_pos].strip()
                        return scope
                
                # If no terminator, take reasonable portion
                words = after_negation.split()
                scope = " ".join(words[:10])  # Take first 10 words
                return scope
        
        return statement

class EnhancedCharacterEmbeddings:
    """Character embedding system for mythology (simplified version without full transformers)"""
    
    def __init__(self):
        # Pre-defined character contexts for better understanding
        self.character_descriptions = {
            "rama": "Noble prince of Ayodhya, avatar of Vishnu, protagonist, husband of Sita, defeats Ravana, follows dharma",
            "bharata": "Brother of Rama, son of Kaikeyi, rules Ayodhya during exile, devoted to Rama, never fights demons",
            "lakshmana": "Devoted younger brother, accompanies Rama in exile, warrior, protector, serves Rama loyally",
            "hanuman": "Powerful monkey warrior, son of Vayu, devoted to Rama, can fly, burns Lanka, finds Sita",
            "sita": "Princess of Mithila, daughter of Janaka, wife of Rama, kidnapped by Ravana, pure and devoted",
            "ravana": "Ten-headed demon king of Lanka, powerful ruler, kidnaps Sita, defeated by Rama",
            "sugriva": "Monkey king of Kishkindha, ally of Rama, brother of Vali, helps in search for Sita"
        }
        
        # Character action compatibility
        self.character_actions = {
            "rama": ["fight", "rescue", "rule", "exile", "marry", "defeat", "mourn", "return"],
            "bharata": ["rule", "wait", "request", "serve", "protect", "govern"],
            "lakshmana": ["accompany", "protect", "fight", "serve", "support"],
            "hanuman": ["fly", "leap", "search", "burn", "serve", "fight", "find"],
            "sita": ["marry", "accompany", "endure", "test", "pure"],
            "ravana": ["kidnap", "rule", "fight", "transform", "deceive"]
        }
    
    def detect_character_confusion(self, text: str, mentioned_character: str) -> Dict[str, Any]:
        """Detect potential character confusion using word overlap analysis"""
        text_lower = text.lower()
        mentioned_char = mentioned_character.lower()
        
        if mentioned_char not in self.character_descriptions:
            return {"confusion_detected": False, "confidence": 0.5}
        
        # Extract actions from text
        text_actions = []
        for char, actions in self.character_actions.items():
            for action in actions:
                if action in text_lower:
                    text_actions.append(action)
        
        # Calculate compatibility scores
        compatibility_scores = {}
        
        for character, char_actions in self.character_actions.items():
            overlap = len(set(text_actions).intersection(set(char_actions)))
            total_actions = len(set(text_actions).union(set(char_actions)))
            score = overlap / max(total_actions, 1)
            compatibility_scores[character] = score
        
        best_match = max(compatibility_scores, key=compatibility_scores.get) if compatibility_scores else mentioned_char
        confusion_detected = mentioned_char != best_match and compatibility_scores.get(best_match, 0) > compatibility_scores.get(mentioned_char, 0) + 0.2
        
        return {
            "confusion_detected": confusion_detected,
            "suggested_character": best_match,
            "confidence": compatibility_scores.get(best_match, 0.5),
            "compatibility_scores": compatibility_scores,
            "mentioned_actions": text_actions
        }

class DynamicRegexGenerator:
    """Generate flexible regex patterns that handle variations in language"""
    
    def __init__(self):
        # Initialize inflect engine for plurals/tenses
        if DYNAMIC_REGEX_AVAILABLE:
            self.inflect_engine = inflect.engine()
            try:
                self.lemmatizer = WordNetLemmatizer()
            except:
                try:
                    nltk.download('wordnet', quiet=True)
                    self.lemmatizer = WordNetLemmatizer()
                except:
                    logger.warning("WordNet lemmatizer initialization failed. Lemmatization disabled.")
                    self.lemmatizer = None
        else:
            self.inflect_engine = None
            self.lemmatizer = None
        
        # Core semantic mappings for Ramayana
        self.semantic_groups = {
            'death_words': ['died', 'dies', 'death', 'perished', 'passed away', 'expired'],
            'grief_words': ['grief', 'sorrow', 'sadness', 'mourning', 'lamentation'],
            'exile_words': ['exile', 'banishment', 'forest', 'departed', 'left'],
            'movement_words': ['went', 'goes', 'traveled', 'journeyed', 'departed'],
            'time_after': ['after', 'following', 'subsequent to', 'when', 'once'],
            'time_before': ['before', 'prior to', 'preceding'],
            'possession': ['had', 'has', 'possessed', 'owned'],
            'being': ['was', 'is', 'were', 'are', 'being'],
            'action_kill': ['killed', 'kills', 'slew', 'slain', 'destroyed'],
            'action_break': ['broke', 'breaks', 'shattered', 'destroyed'],
            'location_from': ['from', 'of', 'belonging to', 'hailing from'],
        }
        
        # Character name variations
        self.character_variants = {
            'dasaratha': ['dasaratha', 'dasharath', 'king dasaratha', 'raja dasaratha'],
            'rama': ['rama', 'raghava', 'ramachandra', 'prince rama', 'lord rama', 'dasharathi'],
            'sita': ['sita', 'seetha', 'janaki', 'vaidehi', 'maithili'],
            'hanuman': ['hanuman', 'anjaneya', 'maruti', 'pawanputra', 'bajrangbali'],
            'ravana': ['ravana', 'dashanan', 'lankesh', 'demon king'],
            'lakshmana': ['lakshmana', 'lakshman', 'saumitri'],
            'bharata': ['bharata', 'bharat'],
            'janaka': ['janaka', 'king janaka', 'raja janaka'],
        }
        
        # Tense transformation rules
        self.tense_patterns = {
            'past_to_present': {
                'died': 'dies',
                'killed': 'kills', 
                'went': 'goes',
                'was': 'is',
                'were': 'are',
                'had': 'has',
                'broke': 'breaks',
                'came': 'comes',
                'gave': 'gives',
                'took': 'takes',
                'found': 'finds',
                'built': 'builds',
                'fought': 'fights',
            }
        }
    
    def generate_dynamic_regex(self, text: str) -> str:
        """Generate a flexible regex pattern that handles multiple variations"""
        if not DYNAMIC_REGEX_AVAILABLE:
            return self._fallback_regex(text)
        
        # Step 1: Extract key components
        components = self._extract_key_components(text.lower())
        
        # Step 2: Generate variations for each component
        variations = self._generate_component_variations(components)
        
        # Step 3: Build flexible regex with optional elements
        regex_pattern = self._build_flexible_pattern(variations)
        
        return regex_pattern
    
    def _fallback_regex(self, text: str) -> str:
        """Fallback regex generation when advanced features unavailable"""
        # Simple word-based pattern
        words = re.findall(r'\b\w+\b', text.lower())
        important_words = [w for w in words if len(w) > 3 and w not in ['that', 'this', 'with', 'from', 'they', 'were', 'been']]
        
        if len(important_words) >= 2:
            pattern = r'.*'.join([rf'\b{re.escape(word)}\b' for word in important_words[:3]])
            return pattern
        else:
            return re.escape(text.lower())
    
    def _extract_key_components(self, text: str) -> Dict[str, List[str]]:
        """Extract meaningful components from text"""
        components = {
            'characters': [],
            'actions': [],
            'objects': [],
            'locations': [],
            'time_markers': [],
            'descriptors': []
        }
        
        words = re.findall(r'\b\w+\b', text)
        
        for word in words:
            # Character detection
            for char_key, variants in self.character_variants.items():
                if any(variant in text for variant in variants):
                    components['characters'].append(char_key)
                    break
            
            # Action detection (verbs)
            if self._is_action_word(word):
                components['actions'].append(word)
            
            # Time marker detection
            if self._is_time_marker(word):
                components['time_markers'].append(word)
            
            # Important descriptors
            if self._is_important_descriptor(word):
                components['descriptors'].append(word)
        
        return components
    
    def _generate_component_variations(self, components: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """Generate variations for each component"""
        variations = {}
        
        for component_type, words in components.items():
            variations[component_type] = []
            
            for word in words:
                word_variations = self._get_word_variations(word, component_type)
                variations[component_type].extend(word_variations)
        
        return variations
    
    def _get_word_variations(self, word: str, component_type: str) -> List[str]:
        """Get all variations of a word"""
        variations = [word]
        
        # Character variations
        if component_type == 'characters' and word in self.character_variants:
            variations.extend(self.character_variants[word])
        
        # Semantic group variations
        for group_name, group_words in self.semantic_groups.items():
            if word in group_words:
                variations.extend(group_words)
                break
        
        # Tense variations
        if component_type == 'actions':
            # Add present tense if past tense
            if word in self.tense_patterns['past_to_present']:
                variations.append(self.tense_patterns['past_to_present'][word])
            
            # Add past tense if present tense
            for past, present in self.tense_patterns['past_to_present'].items():
                if word == present:
                    variations.append(past)
        
        # Remove duplicates and return
        return list(set(variations))
    
    def _build_flexible_pattern(self, variations: Dict[str, List[str]]) -> str:
        """Build a flexible regex pattern"""
        pattern_parts = []
        
        # Build character patterns (high importance)
        if variations.get('characters'):
            char_pattern = self._build_alternatives(variations['characters'])
            pattern_parts.append(char_pattern)
        
        # Build action patterns (high importance)  
        if variations.get('actions'):
            action_pattern = self._build_alternatives(variations['actions'])
            pattern_parts.append(action_pattern)
        
        # Build descriptor patterns (medium importance)
        if variations.get('descriptors'):
            desc_pattern = self._build_alternatives(variations['descriptors'])
            pattern_parts.append(desc_pattern)
        
        # Build time patterns (medium importance)
        if variations.get('time_markers'):
            time_pattern = self._build_alternatives(variations['time_markers'])
            pattern_parts.append(time_pattern)
        
        # Join with flexible matching (allows words in between)
        if len(pattern_parts) >= 2:
            # Use .* for flexible word separation, but limit to reasonable distance
            flexible_pattern = r'.*?'.join(pattern_parts)
            return f'(?=.*{flexible_pattern})'
        elif len(pattern_parts) == 1:
            return pattern_parts[0]
        else:
            # Fallback to simple word matching
            if variations.get('characters'):
                return r'\b' + re.escape(variations['characters'][0]) + r'\b'
            return r'\bunknown\b'  # Default fallback
    
    def _build_alternatives(self, words: List[str]) -> str:
        """Build regex alternatives for a list of words"""
        if not words:
            return ''
        
        # Clean and escape words
        cleaned_words = []
        for word in words:
            # Handle multi-word phrases
            if ' ' in word:
                # For phrases, allow flexible word separation
                phrase_words = word.split()
                phrase_pattern = r'\s+'.join([rf'\b{re.escape(w)}\b' for w in phrase_words])
                cleaned_words.append(phrase_pattern)
            else:
                cleaned_words.append(rf'\b{re.escape(word)}\b')
        
        # Create alternatives
        return f"({'|'.join(cleaned_words)})"
    
    def _is_action_word(self, word: str) -> bool:
        """Check if word is likely an action/verb"""
        action_words = {
            'died', 'dies', 'death', 'killed', 'kills', 'went', 'goes', 'came', 'comes',
            'was', 'is', 'were', 'are', 'had', 'has', 'broke', 'breaks', 'gave', 'gives',
            'took', 'takes', 'found', 'finds', 'built', 'builds', 'fought', 'fights',
            'lived', 'lives', 'ruled', 'rules', 'exiled', 'banished', 'kidnapped',
            'rescued', 'defeated', 'married', 'born', 'adopted'
        }
        return word in action_words
    
    def _is_time_marker(self, word: str) -> bool:
        """Check if word indicates time relationship"""
        time_words = {'after', 'before', 'during', 'when', 'while', 'following', 'preceding'}
        return word in time_words
    
    def _is_important_descriptor(self, word: str) -> bool:
        """Check if word is an important descriptor"""
        descriptors = {
            'grief', 'sorrow', 'exile', 'forest', 'fourteen', 'years', 'golden', 
            'bow', 'bridge', 'ocean', 'lanka', 'ayodhya', 'prince', 'king', 'queen',
            'demon', 'monkey', 'divine', 'weapon', 'head', 'arms', 'jaw', 'ring'
        }
        return word in descriptors

class HybridRamayanaValidator:
    """Complete hybrid validation system combining all advanced techniques"""
    
    def __init__(self):
        self.character_validator = SanskritCharacterValidator()
        self.negation_processor = ReligiousNegationProcessor()
        self.embedding_system = EnhancedCharacterEmbeddings()
        
        # Error pattern tracking
        self.error_patterns = {
            "character_confusion": 0,
            "negation_errors": 0,
            "timeline_errors": 0,
            "relationship_errors": 0
        }
    
    def comprehensive_validation(self, claim: str) -> Dict[str, Any]:
        """Run comprehensive validation using all systems"""
        results = {
            "original_claim": claim,
            "validation_results": {},
            "error_flags": [],
            "corrections": [],
            "confidence_scores": {}
        }
        
        # 1. Character validation
        char_info = self.character_validator.extract_character_info(claim)
        if char_info["characters"]:
            main_character = char_info["characters"][0]
            actions = " ".join(char_info["actions"])
            other_chars = char_info["characters"][1:] if len(char_info["characters"]) > 1 else []
            target = other_chars[0] if other_chars else None
            
            char_validation = self.character_validator.validate_character_action(
                main_character, actions, target, claim
            )
            results["validation_results"]["character_validation"] = char_validation
            results["confidence_scores"]["character"] = char_validation.get("confidence", 0.5)
            
            if not char_validation["valid"]:
                results["error_flags"].append(char_validation.get("error_type", "character_error"))
                results["corrections"].append(char_validation.get("correction", "Character validation failed"))
        
        # 2. Negation processing
        negation_result = self.negation_processor.process_negated_statement(claim)
        results["validation_results"]["negation_analysis"] = negation_result
        
        if negation_result.get("negation_type") != "none":
            results["confidence_scores"]["negation"] = negation_result.get("confidence", 0.5)
            if negation_result.get("statement_validity") is not None:
                results["negation_verdict"] = negation_result["statement_validity"]
                results["negation_reasoning"] = negation_result["reasoning"]
        
        # 3. Character confusion detection
        if char_info["characters"]:
            confusion_result = self.embedding_system.detect_character_confusion(
                claim, char_info["characters"][0]
            )
            results["validation_results"]["confusion_detection"] = confusion_result
            results["confidence_scores"]["confusion"] = confusion_result.get("confidence", 0.5)
            
            if confusion_result["confusion_detected"]:
                results["error_flags"].append("character_confusion")
                results["corrections"].append(
                    f"Possible character confusion: {confusion_result['suggested_character']} "
                    f"may be more appropriate than {char_info['characters'][0]}"
                )
        
        # 4. Calculate overall confidence
        confidences = list(results["confidence_scores"].values())
        results["overall_confidence"] = np.mean(confidences) if confidences else 0.5
        
        # 5. Determine if claim needs special handling
        results["requires_special_handling"] = len(results["error_flags"]) > 0
        results["error_count"] = len(results["error_flags"])
        
        return results
    
    def get_enhanced_verdict(self, validation_results: Dict[str, Any]) -> Tuple[Optional[str], Optional[str], float]:
        """Get enhanced verdict based on validation results"""
        
        # Check for negation verdict first
        if "negation_verdict" in validation_results:
            verdict = "TRUE" if validation_results["negation_verdict"] else "FALSE"
            reasoning = validation_results["negation_reasoning"]
            confidence = validation_results["confidence_scores"].get("negation", 0.8)
            return verdict, reasoning, confidence
        
        # Check for character validation failures
        char_validation = validation_results["validation_results"].get("character_validation", {})
        if not char_validation.get("valid", True):
            return "FALSE", char_validation["reason"], char_validation["confidence"]
        
        # Check for character confusion
        if "character_confusion" in validation_results["error_flags"]:
            return "FALSE", "Character confusion detected - this appears to assign wrong actions to characters", 0.8
        
        # Default: let normal processing handle
        return None, None, 0.5

class SmartRAGClassifier:
    """Enhanced Hybrid RAG + Semantic Pattern system with advanced validation"""
    
    def __init__(self, api_key: str = None, model_name: str = "llama3-8b-8192"):
        if not GROQ_AVAILABLE:
            raise ImportError("Groq library required")
        
        # Initialize Groq
        self.api_key = api_key or 'YOUR_API_KEY_HERE'
        self.client = Groq(api_key=self.api_key)
        self.model_name = model_name
        
        # Initialize advanced validation systems
        self.hybrid_validator = HybridRamayanaValidator()
        
        # Initialize dynamic regex generator if available
        if DYNAMIC_REGEX_AVAILABLE:
            self.regex_generator = DynamicRegexGenerator()
            logger.info("âœ… Dynamic regex generator initialized")
        else:
            self.regex_generator = None
            logger.warning("âš ï¸ Dynamic regex not available - using fallback patterns")
        
        # Rate limiting
        self.last_request_time = 0
        self.min_request_interval = 1.5
        self.max_retries = 3
        
        # Storage for text processing
        self.semantic_patterns = {}    # Critical fallback patterns
        self.pattern_rules = []        # Loaded from supplementary file
        self.text_chunks = []          # REAL chunks from your 7 files
        self.chunk_index = {}          # Search index for chunks
        self.files_loaded = 0
        self.total_chars_processed = 0
        
        # Performance tracking
        self.validation_stats = {
            "character_errors_prevented": 0,
            "negation_errors_prevented": 0,
            "confusion_detected": 0,
            "advanced_validations": 0
        }
        
        # Initialize system
        self._build_critical_patterns()
        self._test_groq_connection()
        self._load_and_process_real_files()
        self._extract_patterns_from_supplementary()
        self._build_search_index()
    
    def _build_critical_patterns(self):
        """Build minimal critical patterns as immediate fallback"""
        
        # Only the most essential patterns for fallback
        critical_patterns = [
            # Essential IRRELEVANT patterns (non-Ramayana content)
            {
                'pattern': r'\bpython\b.*\bprogramming\b',
                'verdict': 'IRRELEVANT',
                'confidence': 0.99,
                'explanation': 'IRRELEVANT: Programming language'
            },
            {
                'pattern': r'\bparis\b.*\bcapital\b.*\bfrance\b',
                'verdict': 'IRRELEVANT',
                'confidence': 0.99,
                'explanation': 'IRRELEVANT: Modern geography'
            },
            # Character confusion patterns
            {
                'pattern': r'\bbharata\b.*\bmeets?\b.*\bsugriva\b',
                'verdict': 'FALSE',
                'confidence': 0.95,
                'explanation': 'FALSE: Bharata never meets Sugriva - this is character confusion'
            },
            {
                'pattern': r'\bbharata\b.*\bfight\b.*\bravana\b',
                'verdict': 'FALSE',
                'confidence': 0.95,
                'explanation': 'FALSE: Bharata never fights Ravana - only Rama does'
            }
        ]
        
        # Store critical patterns
        for i, pattern_info in enumerate(critical_patterns):
            self.semantic_patterns[f"critical_{i}"] = pattern_info
        
        logger.info(f"âœ… Added {len(critical_patterns)} critical fallback patterns with character validation")
    
    def _extract_patterns_from_supplementary(self):
        """Extract pattern rules from supplementary knowledge file"""
        logger.info("ğŸ” Extracting patterns from supplementary knowledge file...")
        
        supplementary_chunks = [
            chunk for chunk in self.text_chunks 
            if 'supplementary' in chunk.get('source', '').lower()
        ]
        
        patterns_found = 0
        for chunk in supplementary_chunks:
            text = chunk['text']
            # Look for pattern sections
            if 'FAST PATTERN RECOGNITION DATABASE' in text or 'VERIFIED TRUE PATTERNS' in text:
                patterns_found += self._parse_pattern_section(text)
        
        logger.info(f"âœ… Loaded {len(self.pattern_rules)} pattern rules from supplementary file")
        
        if patterns_found == 0:
            logger.warning("âš ï¸ No patterns found in supplementary file, using legacy patterns")
            self._build_legacy_patterns()

    def _parse_pattern_section(self, text: str) -> int:
        """Parse pattern section and build rules"""
        lines = text.split('\n')
        current_section = None
        patterns_added = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Check for section headers
            if 'VERIFIED TRUE PATTERNS:' in line:
                current_section = 'TRUE'
                logger.info("ğŸ“ Found TRUE patterns section")
            elif 'VERIFIED FALSE PATTERNS:' in line:
                current_section = 'FALSE'
                logger.info("ğŸ“ Found FALSE patterns section")
            elif 'IRRELEVANT PATTERNS:' in line:
                current_section = 'IRRELEVANT'
                logger.info("ğŸ“ Found IRRELEVANT patterns section")
            elif 'PATTERN SECTION END' in line:
                logger.info("ğŸ“ End of pattern section")
                break
            
            # Parse bullet point patterns
            elif line.startswith('-') and current_section and ':' in line:
                success = self._parse_bullet_pattern(line, current_section)
                if success:
                    patterns_added += 1
        
        return patterns_added

    def _parse_bullet_pattern(self, line: str, verdict: str) -> bool:
        """Parse bullet point pattern format"""
        try:
            # Remove bullet and split on first colon
            line = line[1:].strip()  # Remove the bullet '-'
            
            if ':' not in line or 'confidence' not in line.lower():
                logger.warning(f"âš ï¸ Skipping malformed pattern line: {line[:50]}...")
                return False
            
            # Split on the first occurrence of ': '
            colon_pos = line.find(': ')
            if colon_pos == -1:
                colon_pos = line.find(':')
            
            pattern_text = line[:colon_pos].strip()
            rest = line[colon_pos+1:].strip()
            
            # Extract confidence value
            conf_match = re.search(r'confidence\s+(\d+\.?\d*)', rest)
            confidence = float(conf_match.group(1)) if conf_match else 0.88
            
            # Convert to regex pattern
            regex_pattern = self._text_to_regex(pattern_text)
            
            # Create pattern rule
            pattern_rule = {
                'name': pattern_text,
                'pattern': regex_pattern,
                'verdict': verdict,
                'confidence': confidence,
                'explanation': f'{verdict}: {pattern_text}',
                'source': 'supplementary_file'
            }
            
            self.pattern_rules.append(pattern_rule)
            logger.debug(f"âœ… Added pattern: {pattern_text} -> {verdict}")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error parsing pattern line '{line}': {e}")
            return False

    def _text_to_regex(self, text: str) -> str:
        """Convert natural language pattern to regex using dynamic system with fallback"""
        # Try dynamic regex generator first
        if self.regex_generator:
            try:
                dynamic_pattern = self.regex_generator.generate_dynamic_regex(text)
                if dynamic_pattern and len(dynamic_pattern) > 10:
                    logger.debug(f"ğŸ”„ Dynamic regex: '{text}' -> '{dynamic_pattern}'")
                    return dynamic_pattern
            except Exception as e:
                logger.warning(f"âš ï¸ Dynamic regex failed for '{text}': {e}")
        
        # Fallback to original implementation
        return self._text_to_regex_fallback(text)
    
    def _text_to_regex_fallback(self, text: str) -> str:
        """Original text_to_regex implementation as fallback"""
        text_lower = text.lower()
        
        # Handle specific pattern conversions with high precision
        conversions = {
            # Ravana patterns - CRITICAL FIXES
            'ravana had ten heads and twenty arms': r'\bravana\b.*\b(ten|10)\b.*\bheads?\b.*\b(twenty|20)\b.*\barms?\b',
            'ravana had twenty arms not ten': r'\bravana\b.*\b(twenty|20)\b.*\barms?\b',
            'ravana had twenty arms': r'\bravana\b.*\b(twenty|20)\b.*\barms?\b',
            'ravana had ten arms instead of twenty': r'\bravana\b.*\b(ten|10)\b.*\barms?\b',
            'ravana had ten arms': r'\bravana\b.*\b(ten|10)\b.*\barms?\b',
            
            # Hanuman patterns - CRITICAL FIXES
            'hanuman was son of wind god vayu': r'\bhanuman\b.*\bson\b.*\b(vayu|wind)\b',
            'hanuman was son of vayu': r'\bhanuman\b.*\bson\b.*\b(vayu|wind)\b',
            'hanuman was not son of vayu': r'\bhanuman\b.*\bnot\b.*\bson\b.*\b(vayu|wind)\b',
            
            # Character confusion patterns
            'bharata meets sugriva': r'\bbharata\b.*\bmeets?\b.*\bsugriva\b',
            'bharata fought ravana': r'\bbharata\b.*\bfought?\b.*\bravana\b',
            
            # Exile duration patterns
            'rama was exiled for exactly fourteen years': r'\brama\b.*\bexile\b.*\b(fourteen|14)\b.*\byears?\b',
            'rama was exiled for any number other than fourteen years': r'\brama\b.*\bexile\b.*\b(fifteen|15|thirteen|13|twelve|12|sixteen|16|ten|10|eleven|11)\b.*\byears?\b',
            'fourteen years': r'\b(fourteen|14)\b.*\byears?\b',
            
            # Basic character patterns
            'rama was prince of ayodhya': r'\brama\b.*\bprince\b.*\bayodhya\b',
            'lakshmana was rama\'s younger brother': r'\blakshmana\b.*\b(younger|brother)\b.*\brama\b',
            'lakshmana was not related to rama': r'\blakshmana\b.*\bnot.*\brelated\b.*\brama\b',
            
            # Kumbhakarna sleep patterns
            'kumbhakarna slept for six months and was awake for six months': r'\bkumbhakarna\b.*\bsix\b.*\bmonths?\b',
            'kumbhakarna slept for one day only': r'\bkumbhakarna\b.*\bone\b.*\bday\b',
            
            # Bridge patterns
            'bridge to lanka was built by monkey army': r'\bbridge\b.*\blanka\b.*\bmonkey\b',
            'nala was architect who built bridge to lanka': r'\bnala\b.*\barchitect\b.*\bbridge\b',
            
            # Ramayana structure
            'ramayana has seven kandas not six': r'\bramayana\b.*\bseven\b.*\bkandas?\b',
            'ramayana has six kandas only': r'\bramayana\b.*\bsix\b.*\bkandas?\b',
            
            # Modern/irrelevant patterns
            'python programming language': r'\bpython\b.*\bprogramming\b',
            'paris capital of france': r'\bparis\b.*\bcapital\b.*\bfrance\b',
            'modern technology': r'\bmodern\b.*\btechnology\b',
        }
        
        # Check for exact matches first
        for phrase, regex in conversions.items():
            if phrase in text_lower:
                logger.debug(f"ğŸ¯ Exact conversion: '{phrase}' -> '{regex}'")
                return regex
        
        # Handle partial matches for key terms
        key_terms = {
            'ravana': r'\bravana\b',
            'hanuman': r'\bhanuman\b', 
            'rama': r'\brama\b',
            'bharata': r'\bbharata\b',
            'lakshmana': r'\blakshmana\b',
            'sita': r'\bsita\b',
            'fourteen': r'\b(fourteen|14)\b',
            'twenty': r'\b(twenty|20)\b',
            'ten': r'\b(ten|10)\b',
            'arms': r'\barms?\b',
            'heads': r'\bheads?\b',
            'exile': r'\bexile\b',
            'bridge': r'\bbridge\b',
            'kandas': r'\bkandas?\b'
        }
        
        # Build pattern from key terms found
        words = re.findall(r'\b\w+\b', text_lower)
        regex_parts = []
        
        for word in words:
            if word in key_terms:
                regex_parts.append(key_terms[word])
            elif word not in ['was', 'were', 'the', 'of', 'and', 'in', 'by', 'to', 'for', 'with', 'a', 'an', 'had', 'has']:
                regex_parts.append(f'\\b{re.escape(word)}\\b')
        
        if len(regex_parts) >= 2:
            # Join with flexible matching
            pattern = r'.*'.join(regex_parts[:4])  # Limit to first 4 terms
            logger.debug(f"ğŸ”§ Generated pattern: '{text}' -> '{pattern}'")
            return pattern
        
        # Fallback: escape the entire text
        escaped = re.escape(text_lower)
        logger.debug(f"ğŸ”§ Fallback pattern: '{text}' -> '{escaped}'")
        return escaped

    def _build_legacy_patterns(self):
        """Build essential fallback patterns if file parsing fails"""
        
        # Essential patterns for core functionality
        legacy_patterns = [
            # TRUE patterns
            {
                'name': 'Ravana ten heads twenty arms',
                'pattern': r'\bravana\b.*\b(ten|10)\b.*\bheads?\b.*\b(twenty|20)\b.*\barms?\b',
                'verdict': 'TRUE',
                'confidence': 0.98,
                'explanation': 'TRUE: Ravana had exactly 10 heads and 20 arms',
                'source': 'legacy'
            },
            {
                'name': 'Hanuman son of Vayu',
                'pattern': r'\bhanuman\b.*\bson\b.*\b(vayu|wind)\b',
                'verdict': 'TRUE',
                'confidence': 0.98,
                'explanation': 'TRUE: Hanuman was son of wind god Vayu',
                'source': 'legacy'
            },
            {
                'name': 'Rama prince of Ayodhya',
                'pattern': r'\brama\b.*\bprince\b.*\bayodhya\b',
                'verdict': 'TRUE',
                'confidence': 0.98,
                'explanation': 'TRUE: Rama was prince of Ayodhya',
                'source': 'legacy'
            },
            
            # FALSE patterns
            {
                'name': 'Ravana ten arms',
                'pattern': r'\bravana\b.*\b(ten|10)\b.*\barms?\b',
                'verdict': 'FALSE',
                'confidence': 0.98,
                'explanation': 'FALSE: Ravana had 20 arms, not 10 arms',
                'source': 'legacy'
            },
            {
                'name': 'Bharata meets Sugriva',
                'pattern': r'\bbharata\b.*\bmeets?\b.*\bsugriva\b',
                'verdict': 'FALSE',
                'confidence': 0.98,
                'explanation': 'FALSE: Bharata never meets Sugriva - character confusion error',
                'source': 'legacy'
            },
            
            # IRRELEVANT patterns
            {
                'name': 'Python programming',
                'pattern': r'\bpython\b.*\bprogramming\b',
                'verdict': 'IRRELEVANT',
                'confidence': 0.99,
                'explanation': 'IRRELEVANT: Programming language',
                'source': 'legacy'
            }
        ]
        
        # Add all legacy patterns to pattern_rules
        self.pattern_rules.extend(legacy_patterns)
        
        logger.info(f"âœ… Added {len(legacy_patterns)} legacy pattern rules with character validation")

    def get_system_stats(self) -> Dict[str, Any]:
        """Get enhanced system statistics for display"""
        entity_index_size = len(self.chunk_index.get('entities', {})) if hasattr(self, 'chunk_index') else 0
        avg_chunk_size = np.mean([chunk.get('char_count', 0) for chunk in self.text_chunks]) if self.text_chunks else 0
        
        return {
            'files_loaded': self.files_loaded,
            'total_chunks': len(self.text_chunks),
            'total_chars_processed': self.total_chars_processed,
            'pattern_rules_loaded': len(self.pattern_rules),
            'critical_patterns': len(self.semantic_patterns),
            'entity_index_size': entity_index_size,
            'avg_chunk_size': avg_chunk_size,
            'advanced_features': {
                'character_validation': True,
                'negation_processing': True,
                'confusion_detection': True,
                'transformers_available': TRANSFORMERS_AVAILABLE,
                'dynamic_regex_available': DYNAMIC_REGEX_AVAILABLE
            },
            'validation_stats': self.validation_stats
        }
    
    def _semantic_pattern_check(self, claim: str) -> Optional[Tuple[str, float, str]]:
        """Enhanced pattern checking with character validation"""
        claim_lower = claim.lower()
        claim_normalized = re.sub(r'[^\w\s]', ' ', claim_lower)
        
        # STEP 1: Run advanced validation first
        try:
            validation_results = self.hybrid_validator.comprehensive_validation(claim)
            self.validation_stats["advanced_validations"] += 1
            
            # Check if advanced validation provides verdict
            enhanced_verdict = self.hybrid_validator.get_enhanced_verdict(validation_results)
            if enhanced_verdict[0]:  # If verdict is not None
                verdict, reasoning, confidence = enhanced_verdict
                if "character confusion" in reasoning.lower():
                    self.validation_stats["confusion_detected"] += 1
                if "negation" in reasoning.lower():
                    self.validation_stats["negation_errors_prevented"] += 1
                
                logger.info(f"âš¡ Advanced validation: {verdict} ({confidence:.2f}) - {reasoning[:50]}...")
                return (verdict, confidence, reasoning)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Advanced validation failed: {e}")
        
        # STEP 2: Check patterns loaded from supplementary file
        for rule in self.pattern_rules:
            pattern_regex = rule['pattern']
            
            try:
                if re.search(pattern_regex, claim_normalized, re.IGNORECASE):
                    verdict = rule['verdict']
                    confidence = rule['confidence']
                    explanation = rule['explanation']
                    source = rule.get('source', 'unknown')
                    
                    logger.info(f"âš¡ Pattern match from {source}: {verdict} ({confidence:.2f}) - {rule.get('name', 'unnamed')}")
                    return (verdict, confidence, explanation)
            except re.error as e:
                logger.warning(f"âš ï¸ Invalid regex pattern '{pattern_regex}': {e}")
                continue
        
        # STEP 3: Fallback to critical hardcoded patterns
        for pattern_id, pattern_info in self.semantic_patterns.items():
            pattern_regex = pattern_info['pattern']
            
            try:
                if re.search(pattern_regex, claim_normalized, re.IGNORECASE):
                    verdict = pattern_info['verdict']
                    confidence = pattern_info['confidence']
                    explanation = pattern_info['explanation']
                    
                    logger.info(f"âš¡ Critical fallback pattern match: {verdict} ({confidence:.2f})")
                    return (verdict, confidence, explanation)
            except re.error:
                continue
        
        return None
    
    def _test_groq_connection(self):
        """Test Groq API"""
        try:
            response = self._safe_groq_request([{"role": "user", "content": "Hi"}], max_tokens=3)
            logger.info("âœ… Groq API connected")
        except Exception as e:
            logger.error(f"âŒ Groq connection failed: {e}")
            raise
    
    def _load_and_process_real_files(self):
        """Load and process your 7 text files"""
        data_dir = Path("data")
        
        # Your actual file list
        ramayana_files = [
            "valmiki_ramayan_supplementary_knowledge.txt",  # For patterns only
            "valmiki_ramayan_bala_kanda_book1.txt",
            "valmiki_ramayan_ayodhya_kanda_book2.txt",
            "valmiki_ramayan_aranya_kanda_book3.txt", 
            "valmiki_ramayan_kishkindha_kanda_book4.txt",
            "valmiki_ramayan_sundara_kanda_book5.txt",
            "valmiki_ramayan_yuddha_kanda_book6.txt"
        ]
        
        if not data_dir.exists():
            logger.warning("âš ï¸ Data directory not found. Creating sample chunks.")
            self._create_fallback_chunks()
            return
        
        total_chunks = 0
        
        for filename in ramayana_files:
            file_path = data_dir / filename
            
            if file_path.exists():
                logger.info(f"ğŸ“– Processing {filename}...")
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    file_size = len(content)
                    self.total_chars_processed += file_size
                    
                    if file_size < 100:
                        logger.warning(f"âš ï¸ {filename} too short, skipping")
                        continue
                    
                    # Process into meaningful chunks
                    file_chunks = self._create_meaningful_chunks(content, filename)
                    self.text_chunks.extend(file_chunks)
                    total_chunks += len(file_chunks)
                    self.files_loaded += 1
                    
                    logger.info(f"âœ… {filename}: {len(file_chunks)} chunks, {file_size:,} chars")
                    
                except Exception as e:
                    logger.error(f"âŒ Error loading {filename}: {e}")
            else:
                logger.warning(f"âš ï¸ File not found: {filename}")
        
        logger.info(f"ğŸ‰ Processed {self.total_chars_processed:,} characters into {total_chunks} chunks from {self.files_loaded} files")
        
        if total_chunks == 0:
            logger.warning("âš ï¸ No real files loaded. Creating fallback chunks.")
            self._create_fallback_chunks()
    
    def _create_meaningful_chunks(self, content: str, filename: str) -> List[Dict]:
        """Create meaningful chunks from file content with proper overlap"""
        
        # Set priority based on filename
        priority_map = {
            'supplementary': 3.0,  # For patterns only
            'bala': 4.5,
            'ayodhya': 4.0,
            'aranya': 3.5,
            'kishkindha': 3.0,
            'sundara': 3.0,
            'yuddha': 2.5
        }
        
        priority = 2.0  # default
        for key, value in priority_map.items():
            if key in filename.lower():
                priority = value
                break
        
        chunks = []
        
        # Method 1: Split by double newlines (paragraphs)
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        if not paragraphs:
            # Fallback: split by sentences
            sentences = re.split(r'[.!?]+', content)
            paragraphs = [s.strip() for s in sentences if len(s.strip()) > 50]
        
        chunk_id = 0
        current_chunk = ""
        target_size = 400  # Optimal size for semantic understanding
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # Check if adding this paragraph exceeds target size
            if len(current_chunk) + len(para) > target_size and current_chunk:
                # Save current chunk
                if len(current_chunk.strip()) > 100:  # Minimum meaningful size
                    chunk_data = {
                        'id': f"{filename}_{chunk_id}",
                        'text': current_chunk.strip(),
                        'source': filename,
                        'priority': priority,
                        'char_count': len(current_chunk),
                        'entities': self._extract_entities(current_chunk),
                        'topics': self._extract_topics(current_chunk),
                        'fact_density': self._calculate_fact_density(current_chunk)
                    }
                    chunks.append(chunk_data)
                    chunk_id += 1
                
                # Create overlap for next chunk
                words = current_chunk.split()
                if len(words) > 30:
                    # Take last 25 words as overlap
                    overlap_text = ' '.join(words[-25:])
                    current_chunk = overlap_text + ' ' + para
                else:
                    current_chunk = para
            else:
                # Add to current chunk
                if current_chunk:
                    current_chunk += ' ' + para
                else:
                    current_chunk = para
        
        # Add final chunk
        if len(current_chunk.strip()) > 100:
            chunk_data = {
                'id': f"{filename}_{chunk_id}",
                'text': current_chunk.strip(),
                'source': filename,
                'priority': priority,
                'char_count': len(current_chunk),
                'entities': self._extract_entities(current_chunk),
                'topics': self._extract_topics(current_chunk),
                'fact_density': self._calculate_fact_density(current_chunk)
            }
            chunks.append(chunk_data)
        
        return chunks
    
    def _extract_entities(self, text: str) -> List[str]:
        """Extract Ramayana entities from text"""
        text_lower = text.lower()
        
        entity_patterns = {
            'rama': ['rama', 'raghava', 'dasharathi', 'ramachandra'],
            'sita': ['sita', 'seetha', 'janaki', 'vaidehi', 'maithili'],
            'hanuman': ['hanuman', 'anjaneya', 'maruti', 'pavan putra'],
            'ravana': ['ravana', 'dashanan', 'lankesh'],
            'bharata': ['bharata', 'bharat'],
            'lakshmana': ['lakshmana', 'lakshman', 'saumitri'],
            'dasaratha': ['dasaratha', 'dasharath'],
            'ayodhya': ['ayodhya', 'kosala'],
            'lanka': ['lanka', 'golden city'],
            'janaka': ['janaka', 'videha'],
            'kaikeyi': ['kaikeyi'],
            'kausalya': ['kausalya'],
            'sumitra': ['sumitra'],
            'sugriva': ['sugriva'],
            'vali': ['vali', 'bali'],
            'sampati': ['sampati'],
            'kumbhakarna': ['kumbhakarna'],
            'indrajit': ['indrajit', 'meghanad'],
            'vibhishana': ['vibhishana']
        }
        
        found_entities = []
        for main_entity, variations in entity_patterns.items():
            for variation in variations:
                if variation in text_lower:
                    found_entities.append(main_entity)
                    break
        
        return found_entities
    
    def _extract_topics(self, text: str) -> List[str]:
        """Extract thematic topics from text"""
        text_lower = text.lower()
        
        topic_keywords = {
            'exile': ['exile', 'banishment', 'forest', 'fourteen years'],
            'marriage': ['marriage', 'swayamvara', 'wedding', 'bow'],
            'war': ['war', 'battle', 'fight', 'army'],
            'devotion': ['devotion', 'loyalty', 'faithful', 'dedication'],
            'kidnapping': ['kidnap', 'abduct', 'taken', 'stolen'],
            'kingdom': ['kingdom', 'throne', 'rule', 'king', 'prince'],
            'dharma': ['dharma', 'righteousness', 'duty', 'virtue'],
            'demons': ['demon', 'rakshasa', 'evil', 'asura'],
            'monkeys': ['monkey', 'vanara', 'ape'],
            'bridge': ['bridge', 'setu', 'ocean', 'crossing'],
            'search': ['search', 'find', 'locate', 'looking'],
            'leap': ['leap', 'jump', 'fly', 'cross']
        }
        
        found_topics = []
        for topic, keywords in topic_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                found_topics.append(topic)
        
        return found_topics
    
    def _calculate_fact_density(self, text: str) -> float:
        """Calculate how fact-dense this chunk is"""
        text_lower = text.lower()
        
        # Count factual indicators
        fact_indicators = [
            'was', 'were', 'is', 'are', 'had', 'has', 'did', 'born',
            'killed', 'married', 'ruled', 'went', 'came', 'said',
            'daughter', 'son', 'brother', 'sister', 'king', 'queen'
        ]
        
        fact_count = sum(1 for indicator in fact_indicators if indicator in text_lower)
        word_count = len(text.split())
        
        return fact_count / max(word_count, 1)
    
    def _create_fallback_chunks(self):
        """Create fallback chunks if files not found"""
        fallback_chunks = [
            {
                'id': 'fallback_1',
                'text': 'Rama was the eldest prince of Ayodhya, son of King Dasaratha. He was known for his righteousness and virtue.',
                'source': 'fallback',
                'priority': 5.0,
                'char_count': 150,
                'entities': ['rama', 'ayodhya', 'dasaratha'],
                'topics': ['kingdom'],
                'fact_density': 0.3
            },
            {
                'id': 'fallback_2',
                'text': 'Sita was the daughter of King Janaka of Mithila. She was kidnapped by the demon king Ravana.',
                'source': 'fallback',
                'priority': 5.0,
                'char_count': 120,
                'entities': ['sita', 'janaka', 'ravana'],
                'topics': ['kidnapping'],
                'fact_density': 0.4
            },
            {
                'id': 'fallback_3',
                'text': 'Hanuman was a powerful monkey warrior who could fly and leap great distances. He was devoted to Rama.',
                'source': 'fallback',
                'priority': 5.0,
                'char_count': 130,
                'entities': ['hanuman', 'rama'],
                'topics': ['devotion', 'monkeys'],
                'fact_density': 0.3
            }
        ]
        
        self.text_chunks = fallback_chunks
        self.files_loaded = 1
        logger.info("âœ… Created fallback chunks")
    
    def _build_search_index(self):
        """Build search index with balanced weighting"""
        logger.info("ğŸ” Building balanced search index...")
        
        # Build entity index with balanced weighting
        entity_index = {}
        topic_index = {}
        
        for chunk in self.text_chunks:
            chunk_id = chunk['id']
            
            # Index by entities
            for entity in chunk.get('entities', []):
                if entity not in entity_index:
                    entity_index[entity] = []
                entity_index[entity].append({
                    'chunk_id': chunk_id, 
                    'priority': chunk['priority']
                })
            
            # Index by topics
            for topic in chunk.get('topics', []):
                if topic not in topic_index:
                    topic_index[topic] = []
                topic_index[topic].append({
                    'chunk_id': chunk_id, 
                    'priority': chunk['priority']
                })
        
        # Sort indices by priority
        for entity in entity_index:
            entity_index[entity].sort(key=lambda x: x['priority'], reverse=True)
        
        for topic in topic_index:
            topic_index[topic].sort(key=lambda x: x['priority'], reverse=True)
        
        self.chunk_index = {
            'entities': entity_index,
            'topics': topic_index,
            'chunks_by_id': {chunk['id']: chunk for chunk in self.text_chunks}
        }
        
        # Log stats
        kanda_counts = {}
        for chunk in self.text_chunks:
            source = chunk.get('source', 'unknown')
            kanda_counts[source] = kanda_counts.get(source, 0) + 1
        
        logger.info(f"âœ… Built balanced search index: {len(entity_index)} entities, {len(topic_index)} topics")
        logger.info(f"ğŸ“š Chunk distribution: {kanda_counts}")
    
    def _determine_best_kanda(self, claim_lower: str, entities: List[str], topics: List[str]) -> Dict[str, List[str]]:
        """Determine which Kanda books are most relevant to this query"""
        
        # Define what each Kanda book covers
        kanda_coverage = {
            'bala': {
                'topics': ['birth', 'childhood', 'education', 'bow', 'marriage', 'swayamvara'],
                'entities': ['vasishtha', 'vishwamitra', 'janaka', 'shiva', 'tataka'],
                'keywords': ['born', 'child', 'guru', 'teacher', 'bow', 'break', 'wedding', 'marriage'],
                'events': ['rama birth', 'bow breaking', 'sita marriage', 'education']
            },
            'ayodhya': {
                'topics': ['exile', 'kingdom', 'succession', 'politics'],
                'entities': ['dasaratha', 'kaikeyi', 'bharata', 'manthara'],
                'keywords': ['exile', 'kingdom', 'throne', 'boon', 'fourteen', 'years', 'forest'],
                'events': ['dasaratha death', 'exile beginning', 'bharata regency']
            },
            'aranya': {
                'topics': ['forest', 'demons', 'exile', 'kidnapping'],
                'entities': ['shurpanakha', 'khara', 'maricha', 'ravana'],
                'keywords': ['forest', 'demon', 'golden', 'deer', 'kidnap', 'abduct'],
                'events': ['surpanakha incident', 'golden deer', 'sita kidnapping']
            },
            'kishkindha': {
                'topics': ['monkeys', 'alliance', 'friendship'],
                'entities': ['vali', 'sugriva', 'tara', 'angada'],
                'keywords': ['monkey', 'vanara', 'alliance', 'help', 'friend'],
                'events': ['vali killing', 'monkey alliance', 'search planning']
            },
            'sundara': {
                'topics': ['search', 'lanka', 'reconnaissance'],
                'entities': ['hanuman', 'sampati', 'jambavan'],
                'keywords': ['search', 'find', 'leap', 'ocean', 'lanka', 'ring'],
                'events': ['ocean crossing', 'sita meeting', 'lanka burning']
            },
            'yuddha': {
                'topics': ['war', 'battle', 'bridge', 'victory'],
                'entities': ['indrajit', 'kumbhakarna', 'vibhishana', 'lakshmana'],
                'keywords': ['war', 'battle', 'fight', 'bridge', 'weapon', 'victory'],
                'events': ['bridge building', 'lanka war', 'ravana death']
            }
        }
        
        # Score each Kanda for relevance to this query
        kanda_scores = {}
        
        for kanda, coverage in kanda_coverage.items():
            score = 0
            
            # Entity matches
            entity_matches = sum(1 for entity in entities if entity in coverage['entities'])
            score += entity_matches * 15
            
            # Topic matches  
            topic_matches = sum(1 for topic in topics if topic in coverage['topics'])
            score += topic_matches * 10
            
            # Keyword presence
            keyword_matches = sum(1 for keyword in coverage['keywords'] if keyword in claim_lower)
            score += keyword_matches * 5
            
            # Event context
            event_matches = sum(1 for event in coverage['events'] if any(word in claim_lower for word in event.split()))
            score += event_matches * 8
            
            kanda_scores[kanda] = score
        
        # Sort by relevance
        sorted_kandas = sorted(kanda_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Select primary (top 1-2) and secondary (next 1-2) Kandas
        primary_kandas = [kanda for kanda, score in sorted_kandas[:2] if score > 10]
        secondary_kandas = [kanda for kanda, score in sorted_kandas[2:4] if score > 5]
        
        # Add supplementary as fallback only
        result = {
            'primary': primary_kandas,
            'secondary': secondary_kandas + ['supplementary'],
            'scores': dict(sorted_kandas)
        }
        
        logger.debug(f"ğŸ“Š Kanda relevance scores: {dict(sorted_kandas)}")
        
        return result
    
    def _handle_specific_cases(self, claim_lower: str) -> Optional[Dict[str, List[str]]]:
        """Handle specific cases that need particular Kanda attention"""
        
        specific_cases = {
            # Bridge construction - clearly Yuddha Kanda
            'bridge': {'primary': ['yuddha'], 'secondary': ['sundara', 'supplementary']},
            'setu': {'primary': ['yuddha'], 'secondary': ['sundara', 'supplementary']},
            'nala': {'primary': ['yuddha'], 'secondary': ['kishkindha', 'supplementary']},
            
            # Sleep/Kumbhakarna - Yuddha Kanda
            'kumbhakarna': {'primary': ['yuddha'], 'secondary': ['supplementary']},
            'sleep': {'primary': ['yuddha'], 'secondary': ['supplementary']},
            
            # Marriage/Bow - Bala Kanda
            'swayamvara': {'primary': ['bala'], 'secondary': ['supplementary']},
            'shiva.*bow': {'primary': ['bala'], 'secondary': ['supplementary']},
            
            # Exile details - Ayodhya Kanda
            'kaikeyi.*battle': {'primary': ['ayodhya'], 'secondary': ['supplementary']},
            'dasaratha.*died': {'primary': ['ayodhya'], 'secondary': ['supplementary']},
            
            # Forest incidents - Aranya Kanda
            'golden.*deer': {'primary': ['aranya'], 'secondary': ['supplementary']},
            'maricha': {'primary': ['aranya'], 'secondary': ['supplementary']},
            'tataka': {'primary': ['bala'], 'secondary': ['aranya', 'supplementary']},
            
            # War details - Yuddha Kanda
            'indrajit': {'primary': ['yuddha'], 'secondary': ['supplementary']},
            'brahmastra': {'primary': ['yuddha'], 'secondary': ['supplementary']},
            'shakti.*weapon': {'primary': ['yuddha'], 'secondary': ['supplementary']},
            
            # Monkey kingdom - Kishkindha Kanda
            'vali': {'primary': ['kishkindha'], 'secondary': ['supplementary']},
            'sugriva': {'primary': ['kishkindha'], 'secondary': ['sundara', 'supplementary']},
            'tara': {'primary': ['kishkindha'], 'secondary': ['supplementary']},
            
            # Search operations - Sundara Kanda
            'hanuman.*leap': {'primary': ['sundara'], 'secondary': ['supplementary']},
            'ocean.*cross': {'primary': ['sundara'], 'secondary': ['supplementary']},
            'sampati': {'primary': ['sundara'], 'secondary': ['kishkindha', 'supplementary']},
        }
        
        for pattern, kandas in specific_cases.items():
            if re.search(pattern, claim_lower):
                logger.info(f"ğŸ¯ Specific case matched: {pattern} â†’ {kandas}")
                return kandas
        
        return None
    
    def _smart_rag_retrieval(self, claim: str, top_k: int = 3) -> List[Dict]:
        """Smart RAG retrieval - choose the most relevant Kanda book first"""
        claim_lower = claim.lower()
        claim_words = set(re.findall(r'\b\w+\b', claim_lower))
        
        # Extract query entities and topics
        query_entities = self._extract_entities(claim)
        query_topics = self._extract_topics(claim)
        
        # STEP 1: Check for specific cases first
        specific_case = self._handle_specific_cases(claim_lower)
        if specific_case:
            kanda_relevance = specific_case
        else:
            # STEP 2: General Kanda relevance determination
            kanda_relevance = self._determine_best_kanda(claim_lower, query_entities, query_topics)
        
        # STEP 3: Score chunks with smart source prioritization
        scored_chunks = []
        
        for chunk in self.text_chunks:
            score = 0.0
            source = chunk.get('source', '').lower()
            
            # Smart source weighting based on Kanda relevance
            if any(kanda in source for kanda in kanda_relevance['primary']):
                score += 100.0  # Highest for primary Kanda
                logger.debug(f"ğŸ¯ Primary Kanda boost: {chunk['id']}")
            elif any(kanda in source for kanda in kanda_relevance['secondary']):
                score += 40.0   # Medium for secondary Kanda
                logger.debug(f"ğŸ“– Secondary Kanda boost: {chunk['id']}")
            else:
                score += 5.0   # Low score for other Kandas
            
            # Entity matching (highest weight)
            chunk_entities = chunk.get('entities', [])
            entity_overlap = len(set(query_entities).intersection(set(chunk_entities)))
            score += entity_overlap * 20.0
            
            # Topic matching
            chunk_topics = chunk.get('topics', [])
            topic_overlap = len(set(query_topics).intersection(set(chunk_topics)))
            score += topic_overlap * 15.0
            
            # Text similarity
            chunk_text_lower = chunk['text'].lower()
            chunk_words = set(re.findall(r'\b\w+\b', chunk_text_lower))
            word_overlap = len(claim_words.intersection(chunk_words))
            score += word_overlap * 5.0
            
            # Exact phrase matching
            for phrase in claim.split():
                if len(phrase) > 4 and phrase in chunk_text_lower:
                    score += 10.0
            
            # Priority and fact density
            score += chunk.get('priority', 1.0) * 2.0
            score += chunk.get('fact_density', 0.0) * 3.0
            
            if score > 0:
                scored_chunks.append((chunk, score))
        
        # Sort and return top chunks
        scored_chunks.sort(key=lambda x: x[1], reverse=True)
        final_chunks = [chunk for chunk, score in scored_chunks[:top_k]]
        
        if final_chunks:
            top_score = scored_chunks[0][1] if scored_chunks else 0
            sources_used = [chunk.get('source', 'unknown') for chunk in final_chunks]
            logger.info(f"ğŸ¯ Smart RAG: Using sources {sources_used} (top score: {top_score:.1f})")
            logger.info(f"ğŸ“š Primary Kanda: {kanda_relevance['primary']}, Secondary: {kanda_relevance['secondary']}")
        
        return final_chunks
    
    def _create_rag_prompt_with_reasoning(self, claim: str, chunks: List[Dict], pattern_hint: Optional[Tuple[str, float, str]] = None) -> List[Dict]:
        """Create ENHANCED RAG prompt with bias-neutral religious fact-checking"""
        
        claim_lower = claim.lower()
        
        # Check for non-Ramayana content
        non_ramayana_terms = ['python', 'programming', 'paris', 'france', 'computer', 'technology', 'javascript', 'software']
        is_non_ramayana = any(term in claim_lower for term in non_ramayana_terms)
        
        # Check if it contains ANY Ramayana-related terms
        ramayana_terms = ['rama', 'sita', 'hanuman', 'ravana', 'bharata', 'lakshmana', 'ayodhya', 'lanka', 
                         'vanara', 'monkey', 'demon', 'rakshasa', 'bow', 'exile', 'bridge', 'ocean']
        contains_ramayana_content = any(term in claim_lower for term in ramayana_terms)
        
        # Build pattern hint text if available
        pattern_hint_text = ""
        if pattern_hint:
            suggested_verdict, confidence, explanation = pattern_hint
            pattern_hint_text = f"\nADVANCED VALIDATION RESULT: {suggested_verdict} - {explanation}"
        
        if is_non_ramayana and not contains_ramayana_content:
            # ONLY pure modern topics get IRRELEVANT
            system_prompt = f"""You verify Ramayana facts only.

This claim is about modern topics (programming, cities, technology).{pattern_hint_text}

Format your answer as:
VERDICT: IRRELEVANT
REASONING: [Explain why this is not related to Ramayana]"""
        
            user_prompt = f'Claim: "{claim}"\nProvide verdict and reasoning:'
    
        else:
            # ALL Ramayana-related content gets TRUE/FALSE classification with ENHANCED PROMPTING
            if chunks:
                evidence_parts = []
                for i, chunk in enumerate(chunks[:2]):  # Use top 2 chunks
                    source = chunk.get('source', 'unknown')
                    text = chunk['text'][:300]  # Limit text length
                    evidence_parts.append(f"Evidence {i+1} (from {source}):\n{text}")
                
                evidence = "\n\n".join(evidence_parts)
                
                # ENHANCED BIAS-NEUTRAL PROMPT WITH CHARACTER VALIDATION
                system_prompt = f"""You are a Sanskrit scholar analyzing Valmiki Ramayana with strict adherence to original text.

ANALYSIS FRAMEWORK:
1. Character Verification: Are characters correctly identified?
   - Protagonists: Rama, Sita, Lakshmana
   - Supporting: Bharata, Hanuman, Sugriva  
   - Antagonists: Ravana, Kumbhakarna

2. Relationship Validation: Are character interactions historically accurate?
   - Rama and Lakshmana meet Sugriva (NOT Bharata and Lakshmana)
   - Rama mourns Sita's abduction (NOT Bharata as primary mourner)
   - Hanuman is devoted to Rama (NOT Bharata)

3. Event Verification: Check against Ramayana Kandas
   - Bridge construction: Yuddha Kanda (Nala as architect)
   - Marriage/Bow: Bala Kanda (Sita's swayamvara)
   - Exile: Ayodhya Kanda (Rama exiled for exactly 14 years)

EVIDENCE FROM RAMAYANA:
{evidence}{pattern_hint_text}

CRITICAL VALIDATION RULES:
- Rama was exiled for EXACTLY 14 years - ANY other number is FALSE
- Ravana had EXACTLY 10 heads and 20 arms (not 10 arms) - ANY other number is FALSE  
- Kumbhakarna slept for EXACTLY 6 months and was awake for 6 months
- Hanuman was son of wind god Vayu - this is fundamental fact
- Bharata NEVER meets Sugriva or fights demons - these are Rama's actions

NEGATION HANDLING: For "Not:" statements
- Extract core factual claim
- Verify core fact independently  
- Apply negation logic: "Not: [TRUE FACT]" = FALSE

Format your answer as:
VERDICT: [TRUE/FALSE]
REASONING: [One sentence explanation citing character roles and textual evidence]"""
        
            else:
                # No relevant chunks found - use enhanced knowledge-based prompt
                system_prompt = f"""You are a Sanskrit scholar analyzing Valmiki Ramayana with strict textual adherence.

CRITICAL: This is about Ramayana, so NEVER answer IRRELEVANT. Only TRUE or FALSE.{pattern_hint_text}

CHARACTER CONFUSION PREVENTION:
âœ… Rama: protagonist, meets Sugriva, fights Ravana, exiled for 14 years
âœ… Bharata: supporting character, rules Ayodhya, NEVER meets Sugriva or fights demons
âœ… Hanuman: monkey warrior, devoted to Rama, son of Vayu
âœ… Ravana: 10 heads, 20 arms (not 10 arms), defeated by Rama

EXACT FACTS FROM RAMAYANA:
âœ… Rama: exiled for EXACTLY 14 years
âœ… Ravana: EXACTLY 10 heads and 20 arms (not 10 arms)
âœ… Kumbhakarna: slept 6 months, awake 6 months
âœ… Hanuman: son of wind god Vayu
âœ… Bridge to Lanka: built by monkey army under Nala's architecture

NEGATION PROCESSING:
- "Not: [TRUE FACT]" should be marked FALSE
- "Not: [FALSE FACT]" should be marked TRUE

Format your answer as:
VERDICT: [TRUE/FALSE]
REASONING: [One sentence explanation based on Ramayana knowledge and character roles]"""
        
            user_prompt = f'Claim: "{claim}"\nProvide verdict and reasoning:'
    
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    def _parse_response_with_reasoning(self, response: str, statement: str) -> Tuple[str, str]:
        """Enhanced response parsing with character validation awareness"""
        if not response:
            return self._fallback_classification_with_reasoning(statement)
        
        response_clean = response.strip()
        statement_lower = statement.lower()
        
        # Check if this is actually Ramayana content
        ramayana_terms = ['rama', 'sita', 'hanuman', 'ravana', 'bharata', 'lakshmana', 'ayodhya', 'lanka', 
                         'vanara', 'monkey', 'demon', 'rakshasa', 'bow', 'exile', 'bridge', 'ocean']
        is_ramayana_content = any(term in statement_lower for term in ramayana_terms)
        
        # Try to parse structured response
        verdict = None
        reasoning = None
        
        lines = response_clean.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('VERDICT:'):
                verdict_part = line.replace('VERDICT:', '').strip().upper()
                if verdict_part in ['TRUE', 'FALSE', 'IRRELEVANT']:
                    verdict = verdict_part
            elif line.startswith('REASONING:'):
                reasoning = line.replace('REASONING:', '').strip()
        
        # If we got both, return them
        if verdict and reasoning:
            # CRITICAL FIX: Never return IRRELEVANT for Ramayana content
            if verdict == 'IRRELEVANT' and is_ramayana_content:
                logger.warning(f"Model returned IRRELEVANT for Ramayana content, changing to TRUE")
                verdict = 'TRUE'
                reasoning = "Ramayana-related content classified as true by default"
            return verdict, reasoning
        
        # Fallback parsing - look for keywords
        response_upper = response_clean.upper()
        
        if 'TRUE' in response_upper and 'FALSE' not in response_upper:
            verdict = 'TRUE'
        elif 'FALSE' in response_upper:
            verdict = 'FALSE'
        elif 'IRRELEVANT' in response_upper:
            if is_ramayana_content:
                verdict = 'TRUE'
                reasoning = "Ramayana-related content classified as true by default"
            else:
                verdict = 'IRRELEVANT'
        else:
            return self._fallback_classification_with_reasoning(statement)
        
        # Extract reasoning if not found
        if not reasoning:
            # Take the whole response as reasoning if it's reasonable length
            if len(response_clean) > 10 and len(response_clean) < 200:
                reasoning = response_clean
            else:
                reasoning = "Classification based on Ramayana knowledge"
        
        return verdict, reasoning
    
    def _fallback_classification_with_reasoning(self, statement: str) -> Tuple[str, str]:
        """Enhanced fallback classification with character validation"""
        statement_lower = statement.lower()
        
        # ONLY pure non-Ramayana content gets IRRELEVANT
        non_ramayana_terms = ['python', 'programming', 'paris', 'france', 'computer', 'technology', 'javascript', 'software']
        ramayana_terms = ['rama', 'sita', 'hanuman', 'ravana', 'bharata', 'lakshmana', 'ayodhya', 'lanka', 
                         'vanara', 'monkey', 'demon', 'rakshasa', 'bow', 'exile', 'bridge', 'ocean']
        
        has_non_ramayana = any(term in statement_lower for term in non_ramayana_terms)
        has_ramayana = any(term in statement_lower for term in ramayana_terms)
        
        # IRRELEVANT only for pure modern topics with NO Ramayana content
        if has_non_ramayana and not has_ramayana:
            return 'IRRELEVANT', 'This statement is about modern technology, not Ramayana'
        
        # Enhanced character confusion detection in fallback
        if has_ramayana:
            # Known character confusion patterns
            confusion_patterns = [
                (['bharata', 'meets', 'sugriva'], 'Bharata never meets Sugriva - this is character confusion with Rama'),
                (['bharata', 'fought', 'ravana'], 'Bharata never fought Ravana - only Rama defeats Ravana'),
                (['bharata', 'rescued', 'sita'], 'Bharata never rescued Sita - this is Rama\'s role'),
                (['hanuman', 'betray'], 'Hanuman never betrayed anyone - he was completely loyal to Rama'),
                (['lakshmana', 'not', 'related'], 'Lakshmana was Rama\'s younger brother'),
                (['sita', 'daughter', 'ravana'], 'Sita was daughter of Janaka, not Ravana'),
                (['rama', 'born', 'lanka'], 'Rama was born in Ayodhya, not Lanka'),
                (['rama', 'refuse', 'exile'], 'Rama willingly accepted exile to honor his father\'s word')
            ]
            
            for pattern, reason in confusion_patterns:
                if all(word in statement_lower for word in pattern):
                    self.validation_stats["character_errors_prevented"] += 1
                    return 'FALSE', reason
            
            # Known false numerical patterns
            numerical_errors = [
                (['ravana', 'ten', 'arms'], 'Ravana had 20 arms, not 10 arms'),
                (['rama', 'fifteen', 'years'], 'Rama was exiled for exactly 14 years, not 15'),
                (['rama', 'thirteen', 'years'], 'Rama was exiled for exactly 14 years, not 13'),
                (['kumbhakarna', 'one', 'day'], 'Kumbhakarna slept for 6 months, not one day')
            ]
            
            for pattern, reason in numerical_errors:
                if all(word in statement_lower for word in pattern):
                    return 'FALSE', reason
            
            # Default to TRUE for Ramayana content (conservative approach)
            return 'TRUE', 'Statement appears to be about Ramayana and is generally consistent with the epic'
        
        # For content with neither modern nor Ramayana terms, default to IRRELEVANT
        return 'IRRELEVANT', 'Statement does not appear to be related to Ramayana'

    def _safe_groq_request(self, messages: List[Dict], max_tokens: int = 50, temperature: float = 0.0) -> str:
        """Safe Groq request with retries"""
        
        for attempt in range(self.max_retries):
            try:
                # Rate limiting
                current_time = time.time()
                time_since_last = current_time - self.last_request_time
                if time_since_last < self.min_request_interval:
                    sleep_time = self.min_request_interval - time_since_last
                    time.sleep(sleep_time)
                
                # Make request
                response = self.client.chat.completions.create(
                    messages=messages,
                    model=self.model_name,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                self.last_request_time = time.time()
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                error_str = str(e).lower()
                
                if "rate limit" in error_str or "429" in error_str:
                    retry_delay = 3.0 * (2 ** attempt)
                    logger.warning(f"âš ï¸ Rate limit. Waiting {retry_delay:.1f}s...")
                    time.sleep(retry_delay)
                    continue
                elif "quota" in error_str:
                    logger.error("âŒ API quota exceeded")
                    raise
                else:
                    if attempt < self.max_retries - 1:
                        time.sleep(2)
                        continue
                    else:
                        raise
        
        return ""

    def classify_statement(self, statement: str) -> Dict[str, Any]:
        """ENHANCED main classification with advanced validation"""
        start_time = time.time()
        
        try:
            statement = str(statement).strip()
            if not statement or statement.lower() in ['nan', 'none', '']:
                return {
                    'statement': statement,
                    'verdict': 'ERROR',
                    'reasoning': 'Empty statement provided'
                }
            
            logger.info(f"ğŸ” Processing with advanced validation: '{statement[:60]}...'")
            
            # STEP 1: Advanced validation first (character confusion, negation, etc.)
            pattern_result = self._semantic_pattern_check(statement)
            
            # If advanced validation provides definitive answer, use it
            if pattern_result and pattern_result[1] > 0.85:  # High confidence threshold
                verdict, confidence, explanation = pattern_result
                logger.info(f"âœ… Advanced validation result: {verdict} ({confidence:.2f})")
                return {
                    'statement': statement,
                    'verdict': verdict,
                    'reasoning': explanation,
                    'confidence': confidence,
                    'method': 'advanced_validation'
                }
            
            # STEP 2: Smart RAG retrieval from relevant Kanda files
            search_start = time.time()
            relevant_chunks = self._smart_rag_retrieval(statement, top_k=3)
            search_time = time.time() - search_start
            
            # STEP 3: Create enhanced RAG prompt with pattern hint
            messages = self._create_rag_prompt_with_reasoning(statement, relevant_chunks, pattern_result)
            
            # STEP 4: Get Groq response with reasoning
            groq_start = time.time()
            groq_response = self._safe_groq_request(messages, max_tokens=80, temperature=0.0)
            groq_time = time.time() - groq_start
            
            # STEP 5: Parse response with enhanced reasoning
            label, reasoning = self._parse_response_with_reasoning(groq_response, statement)
            
            # Calculate final confidence
            base_confidence = 0.8
            if pattern_result:
                base_confidence = min(0.95, base_confidence + pattern_result[1] * 0.15)
            
            logger.info(f"âœ… Enhanced result: {label} - '{statement[:40]}...'")
            
            return {
                'statement': statement,
                'verdict': label,
                'reasoning': reasoning,
                'confidence': base_confidence,
                'method': 'enhanced_rag_validation',
                'advanced_validation_used': pattern_result is not None,
                'processing_time': time.time() - start_time
            }
            
        except Exception as e:
            logger.error(f"âŒ Enhanced classification error: {e}")
            return {
                'statement': statement,
                'verdict': 'ERROR',
                'reasoning': f'Processing error: {str(e)}',
                'method': 'error'
            }
    
    def batch_classify_statements(self, statements: List[str], batch_size: int = 10) -> List[Dict[str, Any]]:
        """Enhanced batch processing with validation tracking"""
        logger.info(f"ğŸ”„ Starting ENHANCED batch classification of {len(statements)} statements")
        
        results = []
        total_time = time.time()
        
        # Reset validation stats
        self.validation_stats = {
            "character_errors_prevented": 0,
            "negation_errors_prevented": 0,
            "confusion_detected": 0,
            "advanced_validations": 0
        }
        
        for i in range(0, len(statements), batch_size):
            batch = statements[i:i + batch_size]
            batch_start = time.time()
            
            batch_results = []
            for statement in batch:
                result = self.classify_statement(statement)
                batch_results.append(result)
            
            results.extend(batch_results)
            
            batch_time = time.time() - batch_start
            logger.info(f"âœ… Enhanced Batch {i//batch_size + 1}: {len(batch)} statements in {batch_time:.2f}s")
            
            # Small delay between batches to avoid rate limits
            if i + batch_size < len(statements):
                time.sleep(0.5)
        
        total_elapsed = time.time() - total_time
        logger.info(f"ğŸ‰ Enhanced batch processing complete: {len(statements)} statements in {total_elapsed:.2f}s")
        logger.info(f"ğŸ“Š Validation stats: {self.validation_stats}")
        
        return results
    
    def validate_ramayana_knowledge(self) -> Dict[str, Any]:
        """Enhanced validation with character confusion test cases"""
        logger.info("ğŸ§ª Running ENHANCED Ramayana knowledge validation...")
        
        test_cases = [
            # TRUE cases
            {"statement": "Rama was the prince of Ayodhya", "expected": "TRUE"},
            {"statement": "Ravana had ten heads and twenty arms", "expected": "TRUE"},
            {"statement": "Hanuman was son of wind god Vayu", "expected": "TRUE"},
            {"statement": "Rama was exiled for fourteen years", "expected": "TRUE"},
            {"statement": "Sita was daughter of King Janaka", "expected": "TRUE"},
            {"statement": "Bharata ruled Ayodhya during Rama's exile", "expected": "TRUE"},
            
            # FALSE cases - Enhanced with character confusion detection
            {"statement": "Ravana had ten arms", "expected": "FALSE"},
            {"statement": "Bharata meets Sugriva in the forest", "expected": "FALSE"},  # Character confusion
            {"statement": "Bharata fought against Ravana", "expected": "FALSE"},  # Character confusion
            {"statement": "Hanuman was not son of Vayu", "expected": "FALSE"},
            {"statement": "Rama was exiled for fifteen years", "expected": "FALSE"},
            {"statement": "Lakshmana was not related to Rama", "expected": "FALSE"},
            
            # Negation test cases
            {"statement": "Not: Rama was prince of Ayodhya", "expected": "FALSE"},  # Negation of true fact
            {"statement": "Not: Ravana had ten arms", "expected": "TRUE"},  # Negation of false fact
            
            # IRRELEVANT cases
            {"statement": "Python is a programming language", "expected": "IRRELEVANT"},
            {"statement": "Paris is the capital of France", "expected": "IRRELEVANT"},
        ]
        
        correct = 0
        total = len(test_cases)
        results = []
        
        for test_case in test_cases:
            result = self.classify_statement(test_case["statement"])
            is_correct = result["verdict"] == test_case["expected"]
            
            if is_correct:
                correct += 1
            
            results.append({
                "statement": test_case["statement"],
                "expected": test_case["expected"],
                "actual": result["verdict"],
                "correct": is_correct,
                "reasoning": result["reasoning"],
                "method": result.get("method", "unknown"),
                "advanced_validation": result.get("advanced_validation_used", False)
            })
        
        accuracy = correct / total if total > 0 else 0
        
        # Count advanced validation usage
        advanced_validations = sum(1 for r in results if r["advanced_validation"])
        
        validation_result = {
            "accuracy": accuracy,
            "correct": correct,
            "total": total,
            "advanced_validations_used": advanced_validations,
            "validation_stats": self.validation_stats.copy(),
            "details": results
        }
        
        logger.info(f"ğŸ¯ Enhanced Validation Results: {correct}/{total} correct ({accuracy:.1%})")
        logger.info(f"ğŸ”§ Advanced validations used: {advanced_validations}/{total} cases")
        
        return validation_result
    
    def export_enhanced_configuration(self, output_path: str) -> None:
        """Export enhanced system configuration"""
        config = {
            "system_info": {
                "model_name": self.model_name,
                "files_loaded": self.files_loaded,
                "total_chunks": len(self.text_chunks),
                "total_chars_processed": self.total_chars_processed,
                "pattern_rules_loaded": len(self.pattern_rules),
                "critical_patterns": len(self.semantic_patterns),
                "dynamic_regex_available": DYNAMIC_REGEX_AVAILABLE,
                "transformers_available": TRANSFORMERS_AVAILABLE
            },
            "enhanced_features": {
                "character_validation": True,
                "negation_processing": True,
                "confusion_detection": True,
                "bias_neutral_prompting": True,
                "hybrid_validation": True
            },
            "validation_stats": self.validation_stats.copy(),
            "chunk_distribution": {},
            "pattern_sources": {},
            "entity_coverage": list(self.chunk_index.get('entities', {}).keys()) if hasattr(self, 'chunk_index') else [],
            "topic_coverage": list(self.chunk_index.get('topics', {}).keys()) if hasattr(self, 'chunk_index') else []
        }
        
        # Calculate chunk distribution
        for chunk in self.text_chunks:
            source = chunk.get('source', 'unknown')
            config["chunk_distribution"][source] = config["chunk_distribution"].get(source, 0) + 1
        
        # Calculate pattern sources
        for rule in self.pattern_rules:
            source = rule.get('source', 'unknown')
            config["pattern_sources"][source] = config["pattern_sources"].get(source, 0) + 1
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ Enhanced system configuration exported to {output_path}")
    
    def debug_query_processing(self, query: str) -> Dict[str, Any]:
        """Enhanced debug how a query is processed through the system"""
        logger.info(f"ğŸ” Enhanced debug processing for: '{query}'")
        
        debug_info = {
            "original_query": query,
            "entities_extracted": [],
            "topics_extracted": [],
            "kanda_relevance": {},
            "pattern_matches": [],
            "advanced_validation": {},
            "top_chunks": [],
            "processing_steps": []
        }
        
        # Step 1: Entity and topic extraction
        debug_info["processing_steps"].append("Extracting entities and topics")
        debug_info["entities_extracted"] = self._extract_entities(query)
        debug_info["topics_extracted"] = self._extract_topics(query)
        
        # Step 2: Advanced validation
        debug_info["processing_steps"].append("Running advanced validation")
        try:
            validation_results = self.hybrid_validator.comprehensive_validation(query)
            debug_info["advanced_validation"] = {
                "error_flags": validation_results["error_flags"],
                "corrections": validation_results["corrections"],
                "confidence_scores": validation_results["confidence_scores"],
                "requires_special_handling": validation_results["requires_special_handling"]
            }
        except Exception as e:
            debug_info["advanced_validation"] = {"error": str(e)}
        
        # Step 3: Check pattern matches
        debug_info["processing_steps"].append("Checking semantic patterns")
        pattern_result = self._semantic_pattern_check(query)
        if pattern_result:
            debug_info["pattern_matches"].append({
                "verdict": pattern_result[0],
                "confidence": pattern_result[1],
                "explanation": pattern_result[2]
            })
        
        # Step 4: Kanda relevance
        debug_info["processing_steps"].append("Determining Kanda relevance")
        kanda_relevance = self._determine_best_kanda(
            query.lower(), 
            debug_info["entities_extracted"], 
            debug_info["topics_extracted"]
        )
        debug_info["kanda_relevance"] = kanda_relevance
        
        # Step 5: Retrieve top chunks
        debug_info["processing_steps"].append("Retrieving relevant chunks")
        top_chunks = self._smart_rag_retrieval(query, top_k=3)
        debug_info["top_chunks"] = [
            {
                "id": chunk["id"],
                "source": chunk.get("source", "unknown"),
                "priority": chunk.get("priority", 0),
                "entities": chunk.get("entities", []),
                "topics": chunk.get("topics", []),
                "text_preview": chunk["text"][:100] + "..." if len(chunk["text"]) > 100 else chunk["text"]
            }
            for chunk in top_chunks
        ]
        
        logger.info(f"ğŸ” Enhanced debug complete - found {len(debug_info['top_chunks'])} relevant chunks")
        
        return debug_info

def process_csv_enhanced_rag(input_file: str, output_file: str, api_key: str = None):
    """Process CSV with ENHANCED smart RAG system"""
    try:
        logger.info("ğŸš€ Initializing ENHANCED SMART RAG System...")
        classifier = SmartRAGClassifier(api_key=api_key)
        
        # Display enhanced system stats
        stats = classifier.get_system_stats()
        logger.info(f"ğŸ“Š Enhanced System Stats:")
        logger.info(f"  ğŸ“ Files loaded: {stats['files_loaded']}")
        logger.info(f"  ğŸ“š Total chunks: {stats['total_chunks']}")
        logger.info(f"  ğŸ“„ Characters processed: {stats['total_chars_processed']:,}")
        logger.info(f"  âš¡ Pattern rules from file: {stats['pattern_rules_loaded']}")
        logger.info(f"  ğŸ›¡ï¸ Critical fallback patterns: {stats['critical_patterns']}")
        logger.info(f"  ğŸ¯ Character validation: {stats['advanced_features']['character_validation']}")
        logger.info(f"  ğŸ”„ Negation processing: {stats['advanced_features']['negation_processing']}")
        logger.info(f"  ğŸ§  Confusion detection: {stats['advanced_features']['confusion_detection']}")
        
        # Load CSV
        df = pd.read_csv(input_file)
        logger.info(f"ğŸ“‹ Loaded CSV with {len(df)} rows")
        
        # Auto-detect statement column
        statement_column = None
        for col_name in ['statement', 'claim', 'text', 'sentence']:
            if col_name in df.columns:
                statement_column = col_name
                break
        
        if statement_column is None:
            statement_column = df.columns[0]
        
        logger.info(f"ğŸ“ Using column: '{statement_column}'")
        
        results = []
        advanced_validations = 0
        groq_calls_made = 0
        
        start_time = time.time()
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Enhanced RAG Processing"):
            try:
                statement = str(row[statement_column]).strip()
                
                if not statement or statement.lower() in ['nan', 'none', '']:
                    continue
                
                result = classifier.classify_statement(statement)
                results.append(result)
                
                # Track enhanced method usage
                if result.get('advanced_validation_used', False):
                    advanced_validations += 1
                groq_calls_made += 1
                
                # Progress update every 10 items
                if (idx + 1) % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = (idx + 1) / elapsed
                    eta = (len(df) - idx - 1) / rate if rate > 0 else 0
                    logger.info(f"Progress: {idx+1}/{len(df)} | Rate: {rate:.1f}/s | ETA: {eta:.0f}s | Enhanced: {advanced_validations}")
                    
            except Exception as e:
                logger.error(f"âŒ Error processing row {idx}: {e}")
                results.append({
                    'statement': f"Error in row {idx}",
                    'verdict': 'ERROR',
                    'reasoning': f'Processing error: {str(e)}'
                })
        
        if not results:
            logger.error("âŒ No results generated!")
            return
        
        # Save results with enhanced information
        output_data = []
        for result in results:
            output_data.append({
                'statement': result['statement'],
                'verdict': result['verdict'],
                'reasoning': result['reasoning']
            })
        
        output_df = pd.DataFrame(output_data)
        output_df.to_csv(output_file, index=False)
        logger.info(f"ğŸ’¾ Results saved to {output_file}")
        
        # Print enhanced summary
        print_enhanced_summary(output_df, advanced_validations, groq_calls_made, 
                             time.time() - start_time, stats, classifier.validation_stats)
        
    except Exception as e:
        logger.error(f"âŒ Enhanced processing error: {e}")
        import traceback
        traceback.print_exc()

def process_csv_with_enhanced_validation(input_file: str, output_file: str, validation_output: str = None, api_key: str = None):
    """Process CSV with enhanced validation mode"""
    try:
        logger.info("ğŸš€ Initializing ENHANCED SMART RAG System with Full Validation...")
        classifier = SmartRAGClassifier(api_key=api_key)
        
        # Run enhanced validation first
        validation_results = classifier.validate_ramayana_knowledge()
        logger.info(f"ğŸ¯ Enhanced Validation Results: {validation_results['accuracy']:.1%} accuracy")
        logger.info(f"ğŸ”§ Advanced validations used: {validation_results['advanced_validations_used']} cases")
        
        # Save validation results if requested
        if validation_output:
            validation_df = pd.DataFrame(validation_results['details'])
            validation_df.to_csv(validation_output, index=False)
            logger.info(f"ğŸ“„ Enhanced validation results saved to {validation_output}")
        
        # Process the main CSV
        process_csv_enhanced_rag(input_file, output_file, api_key)
        
        # Export enhanced system configuration
        config_output = output_file.replace('.csv', '_enhanced_config.json')
        classifier.export_enhanced_configuration(config_output)
        
    except Exception as e:
        logger.error(f"âŒ Enhanced validation processing error: {e}")
        import traceback
        traceback.print_exc()

def print_enhanced_summary(df: pd.DataFrame, advanced_validations: int, groq_calls: int, 
                         total_time: float, system_stats: Dict, validation_stats: Dict):
    """Print summary of ENHANCED RAG results"""
    print("\n" + "="*80)
    print("ğŸ¯ ENHANCED SMART RAG WITH ADVANCED CHARACTER & NEGATION VALIDATION")
    print("="*80)
    
    if len(df) == 0:
        print("âŒ No results to display")
        return
    
    # Enhanced System Overview
    print(f"ğŸ—ï¸  ENHANCED SYSTEM OVERVIEW:")
    print(f"  ğŸ“ Files processed: {system_stats['files_loaded']}")
    print(f"  ğŸ“š Text chunks created: {system_stats['total_chunks']:,}")
    print(f"  ğŸ“„ Total characters: {system_stats['total_chars_processed']:,}")
    print(f"  âš¡ Patterns from file: {system_stats['pattern_rules_loaded']}")
    print(f"  ğŸ›¡ï¸ Critical fallback patterns: {system_stats['critical_patterns']}")
    print(f"  ğŸ¯ Character validation: ENABLED")
    print(f"  ğŸ”„ Negation processing: ENABLED")
    print(f"  ğŸ§  Confusion detection: ENABLED")
    print(f"  ğŸ¨ Bias-neutral prompting: ENABLED")
    
    # Enhanced Performance Overview
    print(f"\nğŸ“Š ENHANCED PERFORMANCE METRICS:")
    print(f"  ğŸ“‹ Total statements: {len(df)}")
    print(f"  â±ï¸  Total time: {total_time:.2f}s")
    print(f"  ğŸš€ Processing rate: {len(df)/total_time:.2f} statements/sec")
    print(f"  ğŸ”§ Advanced validations: {advanced_validations} ({advanced_validations/len(df)*100:.1f}%)")
    print(f"  ğŸ¤– Groq API calls: {groq_calls} ({groq_calls/len(df)*100:.1f}%)")
    print(f"  ğŸ¯ Character errors prevented: {validation_stats.get('character_errors_prevented', 0)}")
    print(f"  ğŸ”„ Negation errors prevented: {validation_stats.get('negation_errors_prevented', 0)}")
    print(f"  ğŸ§  Confusion cases detected: {validation_stats.get('confusion_detected', 0)}")
    
    # Classification Results
    label_counts = df['verdict'].value_counts()
    print(f"\nğŸ·ï¸  ENHANCED CLASSIFICATION RESULTS:")
    for label, count in label_counts.items():
        percentage = count/len(df)*100
        if label == "TRUE":
            emoji = "âœ…"
        elif label == "FALSE":
            emoji = "âŒ"
        elif label == "IRRELEVANT":
            emoji = "â–"
        elif label == "ERROR":
            emoji = "ğŸš«"
        else:
            emoji = "â“"
        print(f"  {emoji} {label}: {count} statements ({percentage:.1f}%)")
    
    # Sample results
    print(f"\nğŸ“‹ SAMPLE ENHANCED RESULTS:")
    for i, (_, row) in enumerate(df.head(3).iterrows(), 1):
        statement = row['statement'][:50] + "..." if len(row['statement']) > 50 else row['statement']
        print(f"  {i}. '{statement}' â†’ {row['verdict']}")
        print(f"     Enhanced Reasoning: {row['reasoning']}")
        print()
    
    print(f"\n" + "="*80)
    print("ğŸ‰ ENHANCED SMART RAG WITH ADVANCED VALIDATION COMPLETE!")
    print("âœ… Character confusion detection ACTIVE!")
    print("ğŸ”„ Negation processing with 'Not:' handling ACTIVE!")
    print("ğŸ§  Bias-neutral religious fact-checking ACTIVE!")
    print("ğŸ¯ False positive prevention through character validation!")
    print("="*80)

def main():
    """Enhanced main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Enhanced Smart RAG with Advanced Character & Negation Validation')
    parser.add_argument('--input', type=str, required=True, help='Input CSV file')
    parser.add_argument('--output', type=str, required=True, help='Output CSV file')
    parser.add_argument('--api-key', type=str, help='Groq API key (optional)')
    parser.add_argument('--validate', action='store_true', help='Run enhanced validation and analytics')
    parser.add_argument('--validation-output', type=str, help='Save validation results to CSV')
    parser.add_argument('--debug-query', type=str, help='Debug a specific query')
    
    args = parser.parse_args()
    
    print("ğŸ¯ ENHANCED SMART RAG WITH ADVANCED VALIDATION")
    print("="*70)
    print("ğŸš€ NEW ADVANCED FEATURES:")
    print("  ğŸ§  Character confusion detection (Bharata vs Rama)")
    print("  ğŸ”„ Enhanced negation processing ('Not:' prefix)")
    print("  ğŸ¨ Bias-neutral religious fact-checking prompts") 
    print("  ğŸ›¡ï¸ False positive prevention through validation")
    print("  ğŸ“Š Hybrid validation combining rules + ML")
    print("  ğŸ¯ Smart Kanda selection + advanced patterns")
    print()
    
    try:
        if args.debug_query:
            # Enhanced debug mode
            classifier = SmartRAGClassifier(api_key=args.api_key)
            
            # Run full classification to see all systems in action
            result = classifier.classify_statement(args.debug_query)
            debug_info = classifier.debug_query_processing(args.debug_query)
            
            print("ğŸ” ENHANCED DEBUG RESULTS:")
            print(f"Query: {args.debug_query}")
            print(f"Verdict: {result['verdict']}")
            print(f"Reasoning: {result['reasoning']}")
            print(f"Method: {result.get('method', 'unknown')}")
            print(f"Advanced validation used: {result.get('advanced_validation_used', False)}")
            print(f"Processing time: {result.get('processing_time', 0):.3f}s")
            print(f"Entities found: {debug_info['entities_extracted']}")
            print(f"Topics found: {debug_info['topics_extracted']}")
            print(f"Primary Kandas: {debug_info['kanda_relevance'].get('primary', [])}")
            print(f"Error flags: {debug_info['advanced_validation'].get('error_flags', [])}")
        
        elif args.validate:
            # Enhanced validation mode
            process_csv_with_enhanced_validation(args.input, args.output, args.validation_output, args.api_key)
        else:
            # Enhanced normal processing
            process_csv_enhanced_rag(args.input, args.output, args.api_key)
        
        print("\nğŸ‰ Enhanced Smart RAG processing completed successfully!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Process interrupted by user")
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()